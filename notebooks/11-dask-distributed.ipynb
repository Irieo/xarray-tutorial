{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c247eff-7187-4de9-93a9-1c6a60db569b",
   "metadata": {},
   "source": [
    "<img src=\"../images/dask_horizontal.svg\" align=\"right\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55accf7f-c02b-4aae-b1a4-a7a613df03ba",
   "metadata": {},
   "source": [
    "# Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821b7b0-4c44-4004-bce4-8d704ed26545",
   "metadata": {},
   "source": [
    "## Learning Objectives \n",
    "\n",
    "- Use single machine Dask schedulers\n",
    "- Deploy a local Dask Distributed Cluster and access the diagnostics dashboard\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| Familiarity with Python | Necessary | |\n",
    "| Familiarity with Dask Fundamentals | Necessary | |\n",
    "\n",
    "\n",
    "- **Time to learn**: *25-35 minutes*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f990d-100d-43c9-8fb1-e8876b34d3b4",
   "metadata": {},
   "source": [
    "## Dask Schedulers\n",
    "\n",
    "As we have seen so far, Dask allows you to simply construct graphs of tasks with dependencies, as well as have graphs created automatically for you using functional, Numpy or Xarray syntax on data collections. None of this would be very useful, if there weren't also a way to execute these graphs, in a parallel and memory-aware way. So far we have been calling `thing.compute()` or `dask.compute(thing)` without worrying what this entails. Now we will discuss the options available for that execution, and in particular, the distributed scheduler, which comes with additional functionality.\n",
    "\n",
    "Dask comes with four available schedulers:\n",
    "\n",
    "- \"threaded\" (aka \"threading\"): a scheduler backed by a thread pool\n",
    "- \"processes\": a scheduler backed by a process pool\n",
    "- \"single-threaded\" (aka \"sync\"): a synchronous scheduler, good for debugging\n",
    "- distributed: a distributed scheduler for executing graphs on multiple machines, see below.\n",
    "\n",
    "To select one of these for computation, you can specify at the time of asking for a result, e.g.,\n",
    "```python\n",
    "myvalue.compute(scheduler=\"single-threaded\")  # for debugging\n",
    "```\n",
    "\n",
    "You can also set a default scheduler either temporarily\n",
    "```python\n",
    "with dask.config.set(scheduler='processes'):\n",
    "    # set temporarily for this block only\n",
    "    # all compute calls within this block will use the specified scheduler\n",
    "    myvalue.compute()\n",
    "    anothervalue.compute()\n",
    "```\n",
    "\n",
    "Or globally\n",
    "```python\n",
    "# set until further notice\n",
    "dask.config.set(scheduler='processes')\n",
    "```\n",
    "\n",
    "Let's try out a few schedulers on the Sea Surface Temperature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb03e79-dd85-44cb-affb-8864712cbcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import dask\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b94f32-edbc-4a89-96ce-d92d4de3fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(\"data/\")\n",
    "files = sorted(data_dir.glob(\"tos_Omon_CESM2*\"))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83e9e5-9c43-4b0f-95d5-e759c653b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = xr.open_mfdataset(\n",
    "    sorted(files),\n",
    "    concat_dim='ensemble_member',\n",
    "    combine=\"nested\",\n",
    "    parallel=True,\n",
    "    data_vars=['tos'],\n",
    "    engine=\"netcdf4\",\n",
    "    chunks={'time': 90},\n",
    ")\n",
    "# Add coordinate labels for the newly created `ensemble_member` dimension\n",
    "dset[\"ensemble_member\"] = ['r11i1p1f1', 'r7i1p1f1', 'r8i1p1f1', 'r9i1p1f1']\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414c9f2-98f4-4873-b9c9-badf018fde43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute anomaly\n",
    "gb = dset.tos.groupby('time.month')\n",
    "tos_anom = gb - gb.mean(dim='time')\n",
    "tos_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386bad9-4d1f-4b24-be6f-58d76832093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each of the following gives the same results (you can check!)\n",
    "# any surprises?\n",
    "import time\n",
    "\n",
    "for sch in ['threading', 'processes', 'sync']:\n",
    "    t0 = time.time()\n",
    "    r = tos_anom.compute(scheduler=sch)\n",
    "    t1 = time.time()\n",
    "    print(f\"{sch:>10}, {t1 - t0:0.4f} s; {r.min().data, r.max().data, r.mean().data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1f05d-73b1-4b3d-a91f-e18bd8848ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask.visualize(tos_anom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d9129-8bd8-40d7-b3b9-08c30616096a",
   "metadata": {},
   "source": [
    "### Some Questions to Consider:\n",
    "\n",
    "- How much speedup is possible for this task (hint, look at the graph).\n",
    "- Given how many cores are on this machine, how much faster could the parallel schedulers be than the single-threaded scheduler.\n",
    "- How much faster was using threads over a single thread? Why does this differ from the optimal speedup?\n",
    "- Why is the multiprocessing scheduler so much slower here?\n",
    "\n",
    "The `threaded` scheduler is a fine choice for working with large datasets out-of-core on a single machine, as long as the functions being used release the [Python Global Interpreter Lock (GIL)](https://wiki.python.org/moin/GlobalInterpreterLock) most of the time. NumPy and pandas release the GIL in most places, so the `threaded` scheduler is the default for `dask.array` and `dask.dataframe`. The distributed scheduler, perhaps with `processes=False`, will also work well for these workloads on a single machine.\n",
    "\n",
    "For workloads that do hold the GIL, as is common with `dask.bag` and custom code wrapped with `dask.delayed`, we recommend using the distributed scheduler, even on a single machine. Generally speaking, it's more intelligent and provides better diagnostics than the `processes` scheduler.\n",
    "\n",
    "<div class=\"admonition alert alert-info\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">What Is the Python Global Interpreter Lock (GIL)?</p>\n",
    "    <q>The Python Global Interpreter Lock or GIL, in simple words, is a mutex (or a lock) that allows only one thread to hold the control of the Python interpreter.</q>\n",
    "    <br>\n",
    "    See <a href=\"https://realpython.com/python-gil/\">this blog post</a> for more details on Python GIL.\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "https://docs.dask.org/en/latest/scheduling.html provides some additional details on choosing a scheduler.\n",
    "\n",
    "For scaling out work across a cluster, the distributed scheduler is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038ec01-d8d9-49b6-bca3-2745c84bb9fa",
   "metadata": {},
   "source": [
    "## Making a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d438d-dbe1-4c76-9905-3c46af2183b4",
   "metadata": {},
   "source": [
    "### Simple method\n",
    "\n",
    "The `dask.distributed` system is composed of a single centralized scheduler and one or more worker processes. [Deploying](https://docs.dask.org/en/latest/setup.html) a remote Dask cluster involves some additional effort. But doing things locally is just involves creating a `LocalCluster` object and connecting this object to a `Client` object, which lets you interact with the \"cluster\" (local threads or processes on your machine). For more information see [here](https://docs.dask.org/en/latest/setup/single-distributed.html). \n",
    "\n",
    "<img src=\"../images/Distributed Overview (Light).png\">\n",
    "\n",
    "Note that `LocalCluster()` takes a lot of optional [arguments](https://distributed.dask.org/en/latest/local-cluster.html#api), to configure the number of processes/threads, memory limits and other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d8409-4aaf-444b-9b20-e67e9999c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Setup a local cluster.\n",
    "# By default this sets up 1 worker per CPU core\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531d51f-3b21-4aa7-9beb-a8b0a550061b",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "This code\n",
    "\n",
    "```python\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "```\n",
    "\n",
    "is equivalent to \n",
    "\n",
    "```python\n",
    "client = Client()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c70a4a-437b-4126-ace8-90bfffa2b8e5",
   "metadata": {},
   "source": [
    "If you aren't in jupyterlab and using the `dask-labextension`, be sure to click the `Dashboard` link to open up the diagnostics dashboard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1471997-4a70-40a3-bb52-13e2a0d7f498",
   "metadata": {},
   "source": [
    "## Distributed Dask clusters for HPC and Cloud environments\n",
    "\n",
    "Dask can be deployed on distributed infrastructure, such as a an HPC system or a cloud computing system. There is a growing ecosystem of Dask deployment projects that faciliate easy deployment and scaling of Dask clusters on a wide variety of computing systems.\n",
    "\n",
    "### HPC\n",
    "\n",
    "#### Dask Jobqueue (https://jobqueue.dask.org/)\n",
    "\n",
    "- `dask_jobqueue.PBSCluster`\n",
    "- `dask_jobqueue.SlurmCluster`\n",
    "- `dask_jobqueue.LSFCluster`\n",
    "- etc.\n",
    "\n",
    "#### Dask MPI (https://mpi.dask.org/)\n",
    "\n",
    "- `dask_mpi.initialize`\n",
    "\n",
    "### Cloud\n",
    "\n",
    "#### Dask Kubernetes (https://kubernetes.dask.org/)\n",
    "\n",
    "- `dask_kubernetes.KubeCluster`\n",
    "\n",
    "#### Dask Cloud Provider (https://cloudprovider.dask.org)\n",
    "\n",
    "- `dask_cloudprovider.FargateCluster`\n",
    "- `dask_cloudprovider.ECSCluster`\n",
    "- `dask_cloudprovider.ECSCluster`\n",
    "\n",
    "#### Dask Gateway (https://gateway.dask.org/)\n",
    "\n",
    "- `dask_gateway.GatewayCluster`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3f65e-6836-4fba-9df7-58fe9f48aea4",
   "metadata": {},
   "source": [
    "## Executing with the distributed client\n",
    "\n",
    "Consider some calculation, such as we've used before, where we computed anomaly per ensemble member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a4027-e21a-4b47-b6a3-349d0c8155be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tos_anom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a07be93-4b44-4def-85fb-e510306a8f52",
   "metadata": {},
   "source": [
    "By default, creating a `Client` makes it the default scheduler. Any calls to `.compute` will use the cluster your `client` is attached to, unless you specify otherwise, as above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d949a-25a8-40a0-bac2-a43ac1341056",
   "metadata": {},
   "source": [
    "The tasks will appear in the web UI as they are processed by the cluster and, eventually, a result will be printed as output of the cell above. Note that the kernel is blocked while waiting for the result.\n",
    "\n",
    "You can also see a simplified version of the graph being executed on Graph pane of the dashboard, so long as the calculation is in-flight.\n",
    "\n",
    "\n",
    "Let's return to the anomaly computation from before, and see what happens on the dashboard (you may wish to have both the notebook and dashboard side-by-side). How does this perform compared to before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ada56-dff4-4fe0-9b38-0dc7342adf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tos_anom.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7fd256-fa96-4c7d-b981-90d31f043e03",
   "metadata": {},
   "source": [
    "In this particular case, this should be as fast or faster than the best case, threading, above. Why do you suppose this is? You should start your reading [here](https://distributed.dask.org/en/latest/index.html#architecture), and in particular note that the distributed scheduler was a complete rewrite with more intelligence around sharing of intermediate results and which tasks run on which worker. This will result in better performance in *some* cases, but still larger latency and overhead compared to the threaded scheduler, so there will be rare cases where it performs worse. Fortunately, the dashboard now gives us a lot more [diagnostic information](https://distributed.dask.org/en/latest/diagnosing-performance.html). Look at the Profile page of the dashboard to find out what takes the biggest fraction of CPU time for the computation we just performed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a4082d-b8a9-4843-8f93-8ff1f035deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42fc1e-6c86-4568-8a95-d03e00cdf8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark --time --python --updated --iversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d405f-ed11-4e00-8eab-f1382b2848d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf640f8-673e-4f13-a6e2-9b5ff6aa315e",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "If all you want to do is execute computations created using delayed, or run calculations based on the higher-level data collections, then that is about all you need to know to scale your work up to cluster scale. However, there is more detail to know about the distributed scheduler that will help with efficient usage. See this tutorial on advanced features of Distributed: https://tutorial.dask.org/06_distributed_advanced.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ebf3a-edab-4cdd-9f85-f8bba94fb000",
   "metadata": {},
   "source": [
    "## Resources and references\n",
    "\n",
    "* Reference\n",
    "    *  [Dask Docs](https://dask.org/)\n",
    "    *  [Dask Blog](https://blog.dask.org/)\n",
    "    *  [Xarray Docs](https://xarray.pydata.org/)\n",
    "  \n",
    "*  Ask for help\n",
    "    *   [`dask`](http://stackoverflow.com/questions/tagged/dask) tag on Stack Overflow, for usage questions\n",
    "    *   [github discussions (dask):](https://github.com/dask/dask/discussions) for general, non-bug, discussion, and usage questions\n",
    "    *   [github issues (dask): ](https://github.com/dask/dask/issues/new) for bug reports and feature requests\n",
    "     *   [github discussions (xarray): ](https://github.com/pydata/xarray/discussions) for general, non-bug, discussion, and usage questions\n",
    "    *   [github issues (xarray): ](https://github.com/pydata/xarray/issues/new) for bug reports and feature requests\n",
    "    \n",
    "* Pieces of this notebook are adapted from the following sources\n",
    "  * https://github.com/dask/dask-tutorial/blob/main/05_distributed.ipynb\n",
    "  * https://github.com/xarray-contrib/xarray-tutorial/blob/master/scipy-tutorial/05_intro_to_dask.ipynb\n",
    "  \n",
    "  \n",
    "  \n",
    " <div class=\"admonition alert alert-success\">\n",
    "     <p class=\"title\" style=\"font-weight:bold\">Previous: <a href=\"./10-dask-and-xarray.ipynb\">Dask and Xarray</a></p>\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
